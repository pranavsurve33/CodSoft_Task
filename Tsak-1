# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


!pip install scikeras


import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten
from scipy.stats import reciprocal
from keras import backend as K
from skopt.space import Real, Categorical, Integer
from skopt.plots import plot_objective, plot_histogram, plot_evaluations
import skopt
from scikeras.wrappers import KerasClassifier
sns.set_style("darkgrid")

import os
print(os.listdir("../input/"))
df_train = pd.read_csv("/kaggle/input/fraud-detection/fraudTrain.csv")
df_test = pd.read_csv("/kaggle/input/fraud-detection/fraudTest.csv")

#check for missing data - NO MISSING DATA
df_train.isnull().sum()

from datetime import datetime

#convert series object to datetime object
df_test.trans_date_trans_time = pd.to_datetime(df_test.trans_date_trans_time)

#extra datetime elements
df_test['year'] = df_test.trans_date_trans_time.dt.year
df_test['month'] = df_test.trans_date_trans_time.dt.month
df_test['day'] = df_test.trans_date_trans_time.dt.day
df_test['hour'] = df_test.trans_date_trans_time.dt.hour
df_test['date'] = df_test.trans_date_trans_time.dt.date
df_test['week_day'] = df_test.trans_date_trans_time.dt.dayofweek

df_test.dob = pd.to_datetime(df_test.dob)
df_test['age'] = (df_test.trans_date_trans_time - df_test.dob).dt.days // 365
df_test['birth_month'] = df_test.dob.dt.month

from datetime import datetime

#convert series object to datetime object
df_train.trans_date_trans_time = pd.to_datetime(df_train.trans_date_trans_time)

#extra datetime elements
df_train['year'] = df_train.trans_date_trans_time.dt.year
df_train['month'] = df_train.trans_date_trans_time.dt.month
df_train['day'] = df_train.trans_date_trans_time.dt.day
df_train['hour'] = df_train.trans_date_trans_time.dt.hour
df_train['date'] = df_train.trans_date_trans_time.dt.date
df_train['week_day'] = df_train.trans_date_trans_time.dt.dayofweek

df_train.dob = pd.to_datetime(df_train.dob)
df_train['age'] = (df_train.trans_date_trans_time - df_train.dob).dt.days // 365
df_train['birth_month'] = df_train.dob.dt.month

len(df_train['job'].unique())

#check distribution of outcome variable, 'is_fraud'
plt.figure(figsize=(8, 6))
sns.countplot(x='is_fraud', data=df_train)
ax = sns.countplot(x='is_fraud', data=df_train)
plt.title('Distribution of Fraudulent Transactions in the TRAINING data set')
plt.xlabel('Is Fraud')
plt.ylabel('Count')
total = len(df_train['is_fraud'])
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height() / total)
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.text(x, y, percentage, ha='center', va='bottom', fontsize=12, color='black')

plt.show()

#check distribution of outcome variable, 'is_fraud'
plt.figure(figsize=(8, 6))
sns.countplot(x='is_fraud', data=df_test)
ax = sns.countplot(x='is_fraud', data=df_test)
plt.title('Distribution of Fraudulent Transactions in the TEST data set')
plt.xlabel('Is Fraud')
plt.ylabel('Count')
total = len(df_test['is_fraud'])
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height() / total)
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.text(x, y, percentage, ha='center', va='bottom', fontsize=12, color='black')

plt.show()

sum(df_train['is_fraud'] == 1)/df_train.shape[0]

not_fraud = df_train[df_train['is_fraud'] == 0]
fraud = df_train[df_train['is_fraud'] == 1]

plt.figure(figsize=(7,7))
sns.kdeplot(not_fraud.amt, fill=True, label='Not Fraud')
sns.kdeplot(fraud.amt, fill=True, label='Fraud')
plt.ylabel('Frequency')
plt.xlabel('Transaction Amount')
plt.xlim(0, fraud.amt.max())
plt.title('Distribution of Transaction Amount by Is_Fraud')
plt.legend()

plt.figure(figsize=(7,7))
sns.kdeplot(not_fraud.age, fill=True, label='Not Fraud')
sns.kdeplot(fraud.age, fill=True, label='Fraud')
plt.ylabel('Frequency')
plt.xlabel('Age')
plt.xlim(0, fraud.age.max())
plt.title('Distribution of Transaction Amount by Is_Fraud')
plt.legend()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='city_pop', y='amt', hue='is_fraud',alpha=0.5, data=df_train)
plt.title('Transaction Amount vs. City Population and Fraud (Training)')
plt.xlabel('City Population')
plt.ylabel('Transaction Amount')
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='city_pop', y='amt', hue='is_fraud',alpha=0.5, data=df_test)
plt.title('Transaction Amount vs. City Population and Fraud (Test)')
plt.xlabel('City Population')
plt.ylabel('Transaction Amount')
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(12, 6))
ax = sns.histplot(data=df_train, x='category', hue='is_fraud', multiple='stack', stat='percent')
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.2f}%', (p.get_x() + p.get_width() / 2., height),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')
plt.title('Distribution of Categories by Fraud (Training)')
plt.xlabel('Category')
plt.ylabel('Percentage')
plt.xticks(rotation=45, ha="right")
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(12, 6))
ax = sns.histplot(data=df_test, x='category', hue='is_fraud', multiple='stack', stat='percent')
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.2f}%', (p.get_x() + p.get_width() / 2., height),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')
plt.title('Distribution of Categories by Fraud (Test)')
plt.xlabel('Category')
plt.ylabel('Percentage')
plt.xticks(rotation=45, ha="right")
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df_train, x="hour", hue="is_fraud", dodge = False, palette='pastel')
plt.title('Hourly Distribution of Fraudulent Transactions (Training)')
plt.xlabel('Hour')
plt.ylabel('Count')
total = len(df_train['is_fraud'])
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height() / total)
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.text(x, y, percentage, ha='center', va='bottom', fontsize=8, color='black')
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(data=df_train, x="hour", hue="is_fraud", multiple='stack')
plt.title('Hourly Distribution of Fraudulent Transactions')
plt.xlabel('Hour')
plt.ylabel('Count')
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(10, 6))
ax = sns.countplot(data=df_test, x="hour", hue="is_fraud", dodge = False, palette='pastel')
plt.title('Hourly Distribution of Fraudulent Transactions (Test)')
plt.xlabel('Hour')
plt.ylabel('Count')
total = len(df_train['is_fraud'])
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height() / total)
    x = p.get_x() + p.get_width() / 2
    y = p.get_height()
    ax.text(x, y, percentage, ha='center', va='bottom', fontsize=8, color='black')
plt.legend(title='Is Fraud')
plt.show(

plt.figure(figsize=(10, 6))
sns.countplot(x='week_day', hue='is_fraud', data=df_train, dodge = False)
plt.title('Day-wise Distribution of Fraudulent Transactions')
plt.xlabel('Day of Week')
plt.ylabel('Count')
plt.xticks([0, 1, 2, 3, 4, 5, 6], ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(10, 6))
ax = sns.countplot(x='week_day', hue='is_fraud', data=df_train, dodge=False, palette='pastel')

total_height = len(df_train)
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{(height / total_height) * 100:.2f}%', (p.get_x() + p.get_width() / 2., height),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Day-wise Distribution of Fraudulent Transactions (Training)')
plt.xlabel('Day of Week')
plt.ylabel('Count')
plt.xticks([0, 1, 2, 3, 4, 5, 6], ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(10, 6))
ax = sns.countplot(x='week_day', hue='is_fraud', data=df_test, dodge=False, palette='pastel')

total_height = len(df_train)
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{(height / total_height) * 100:.2f}%', (p.get_x() + p.get_width() / 2., height),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Day-wise Distribution of Fraudulent Transactions (Test)')
plt.xlabel('Day of Week')
plt.ylabel('Count')
plt.xticks([0, 1, 2, 3, 4, 5, 6], ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
plt.legend(title='Is Fraud')
plt.show()

#Astrology plot
plt.figure(figsize=(10, 6))
sns.countplot(x='birth_month', hue='is_fraud', data=df_train)
plt.title('Distribution of Fraudulent Transactions by Birth Month')
plt.xlabel('Hour')
plt.ylabel('Count')
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(10, 6))
ax = sns.countplot(x='birth_month', hue='is_fraud', data=df_train, dodge=False, palette='pastel')

total_height = len(df_train)
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{(height / total_height) * 100:.2f}%', (p.get_x() + p.get_width() / 2., height),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Distribution of Fraudulent Transactions by Birth Month (Training)')
plt.xlabel('Birth Month')
plt.ylabel('Count')
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(10, 6))
ax = sns.countplot(x='birth_month', hue='is_fraud', data=df_test, dodge=False, palette='pastel')

total_height = len(df_train)
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{(height / total_height) * 100:.2f}%', (p.get_x() + p.get_width() / 2., height),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points')

plt.title('Distribution of Fraudulent Transactions by Birth Month (Test)')
plt.xlabel('Birth Month')
plt.ylabel('Count')
plt.legend(title='Is Fraud')
plt.show()

plt.figure(figsize=(10, 8))
plt.scatter(df_train['long'], df_train['lat'], c=df_train['is_fraud'], cmap='coolwarm', alpha=0.5)
plt.title('Geographical Distribution of Transactions and Fraud (Training)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

plt.figure(figsize=(10, 8))
plt.scatter(df_test['long'], df_test['lat'], c=df_test['is_fraud'], cmap='coolwarm', alpha=0.5)
plt.title('Geographical Distribution of Transactions and Fraud (Test)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

plt.figure(figsize=(10, 8))
plt.scatter(df_train['merch_long'], df_train['merch_lat'], c=df_train['is_fraud'], cmap='coolwarm', alpha=0.5)
plt.title('Geographical Distribution of Merchants and Fraud (Training)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

plt.figure(figsize=(10, 8))
plt.scatter(df_test['merch_long'], df_test['merch_lat'], c=df_test['is_fraud'], cmap='coolwarm', alpha=0.5)
plt.title('Geographical Distribution of Merchants and Fraud (Test)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()

list(df_train.columns)

numeric_columns = df_train[['amt', 'city_pop', 'unix_time', 'merch_lat', 'merch_long', 'age', 'lat', 'long']]
corr_matrix = numeric_columns.corr()
sns.heatmap(corr_matrix, annot=True,fmt='.3f',cmap="YlGnBu")

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.preprocessing import StandardScaler

#Example of Label Encoding (high dimensionality features)
#label_encoder = LabelEncoder()
#label_cols = ['cc_num']
#label_encoded_train = label_encoder.fit_transform(df_train[label_cols])
#label_encoded_test = label_encoder.fit_transform(df_test[label_cols])              

#One-Hot Encoding (low dimensionality features)
hot_encoder = OneHotEncoder(sparse_output=False)
hot_cols = ['category', 'gender'] #removed 'jobs' to spare dimensionality
hot_encoded_train = hot_encoder.fit_transform(df_train[hot_cols])
hot_encoded_test = hot_encoder.fit_transform(df_test[hot_cols])

#Feature Scaling
scaler = StandardScaler()
numerical_cols = ['amt', 'lat', 'long','city_pop', 'unix_time', 'age']
scaled_train_features = scaler.fit_transform(df_train[numerical_cols])
scaled_test_features = scaler.transform(df_test[numerical_cols])

#Create train and test sets.
train_X = pd.concat([pd.DataFrame(hot_encoded_train), pd.DataFrame(scaled_train_features)], axis=1)
test_X = pd.concat([pd.DataFrame(hot_encoded_test), pd.DataFrame(scaled_test_features)], axis=1)

# Define target variables
train_Y = df_train['is_fraud']
test_Y = df_test['is_fraud']

input_dim = train_X.shape[1]

x_tensor = tf.constant(train_X, dtype=tf.float32)
y_tensor = tf.cast(train_Y, tf.float32)

model = Sequential([
    Dense(22, activation='relu'), 
    Dropout(0.2), 
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=tf.keras.metrics.F1Score()
             )

history = model.fit(x_tensor, y_tensor, epochs=5);

model.summary()

hidden = model.layers[0]
hidden.name
weights, biases = hidden.get_weights()
plt.figure(figsize=(8,8))
plt.imshow(weights.T)

plt.plot(pd.DataFrame(history.history['f1_score']), label='f1_score')
plt.grid(True)
plt.ylabel('F1')
plt.xlabel('Epoch')
plt.gca().set_ylim(0, 1)
plt.show()

def build_model(n_hidden=1, n_neurons=22, droupout_rate=0.2, input_shape=(input_dim)):
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))
    for layer in np.arange(n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation="relu"))
        model.add(tf.keras.layers.Dropout(droupout_rate))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=tf.keras.metrics.F1Score())
    return model

keras_clf = KerasClassifier(build_model, n_hidden=None, n_neurons=None, droupout_rate=None)

################################
##THIS IS WHERE THE ISSUES ARE##
################################

#Hidden layers
#Some practitioners use a rule of thumb, like having a number of neurons 
#in a hidden layer between the number of input and output neurons, or using a geometric mean.
#For example, (number of input neurons + number of output neurons) / 2.
np.int = np.int64
param_distribs = {
    "n_hidden": (1, 4, 'int'),
    "n_neurons": (1, 33, 'int'),
    "droupout_rate": (0.0, 0.3, 'uniform'),
}

# Create an instance of BayesSearchCV
bayes_search = skopt.BayesSearchCV(
    keras_clf,
    param_distribs,
    n_iter=50,  # Number of parameter settings that are sampled
    cv=3,  # Number of cross-validation folds
    verbose=2
)

# Fit the BayesSearchCV instance to the data
bayes_search.fit(x_tensor, y_tensor, epochs=5,callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])

# Print the best hyperparameters found
print("Best hyperparameters:", bayes_search.best_params_)

#build model with the best parameters
model_best = build_model(model_params['n_hidden'], model_params['n_neurons'], model_params['droupout_rate'])
model_best.summary()

# Evaluate the best model on the test set
f1_score = bayes_search.score(test_X, test_Y)
print("F1 of the Best Model:", f1_score)

colors = bayes_search_cv.optimizer_results_[0]['func_vals']
points = np.asarray(bayes_search_cv.optimizer_results_[0]['x_iters'])
points.shape

idx_x = [0,0,1]
idx_y = [1,2,2]
labels = ['dropout_rate','n_hidden','n_neurons']
for i in range(3):
    plt.figure(figsize=(4,3))
    plt.grid(True)
    plt.scatter(points[:,idx_x[i]],points[:,idx_y[i]], c=colors)
    plt.xlabel(labels[idx_x[i]])
    plt.ylabel(labels[idx_y[i]])
    plt.title('Loss from Bayes Optimization')
    plt.colorbar()

history = model.fit(x_tensor, y_tensor, epochs=150);

plt.figure(figsize=(12,5))
plt.plot(pd.DataFrame(history.history['f1_score']), label='f1_score')
plt.grid(True)
plt.ylabel('F1')
plt.xlabel('Epoch')
plt.gca().set_ylim(0, 1)
plt.show()
